# The configuration specific for SPLADE_DistilMSE
id: splade_distilMSE
title: 'SPLADE_DistilMSE: SPLADEv2 trained with the distillated triplets'
description: |
    Training data from: https://github.com/sebastian-hofstaetter/neural-ranking-kd

    From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models
    More Effective (Thibault Formal, Carlos Lassance, Benjamin Piwowarski,
    St√©phane Clinchant). 2022. https://arxiv.org/abs/2205.04733

gpu: true
base_hf_id: distilbert-base-uncased

indexation:
    requirements: duration=1 days & cpu(mem=16G)
    training_requirements: duration=2 days & cuda(mem=24G) & cpu(mem=16G)

splade:
    optimization:
        steps_per_epoch: 128
        # maybe it is too large for a gpu of 24G
        batch_size: 96
        # 150k steps for training
        max_epochs: 1200
        num_warmup_steps: 6000
        lr: 2.0e-5
        re_no_l2_regularization: [] # All parameters have L2 regularization in SPLADE

    model: splade_max
    dataset: hofstaetter_kd_hard_negatives
    # validation for each (steps_per_epoch * validation interval) steps
    validation_interval: 40
    lambda_q: 0.5
    lambda_d: 0.4
    lamdba_warmup_steps: 50000
    requirements: duration=2 days & cuda(mem=24G)

retrieval:
    requirements: duration=2 days & cuda(mem=24G) & cpu(mem=16G)
    topK: 1000
    retTopK: 50
    batch_size_full_retriever: 200
