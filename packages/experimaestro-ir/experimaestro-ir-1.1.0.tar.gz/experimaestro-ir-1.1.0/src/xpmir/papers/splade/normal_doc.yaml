# The configuration specific for SPLADE_doc
id: splade_doc
title: 'SPLADE_doc: SPLADEv2 model with document encoder only'
description: |
    SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval
    (Thibault Formal, Carlos Lassance, Benjamin Piwowarski, St√©phane Clinchant).
    2021. https://arxiv.org/abs/2109.10086

gpu: true
base_hf_id: distilbert-base-uncased

indexation:
    requirements: duration=2 days & cpu(mem=2G)
    training_requirements: duration=4 days & cuda(mem=24G)
    indexspec: OPQ4_16,IVF65536_HNSW32,PQ4
    faiss_max_traindocs: 800_000

splade:
    optimization:
        steps_per_epoch: 128
        # maybe it is too large for a gpu of 24G
        batch_size: 48
        # About 50k steps for training
        max_epochs: 400
        num_warmup_steps: 6000
        lr: 2.0e-5

    model: splade_doc
    validation_interval: 8
    lambda_q: 0
    lambda_d: 1.0e-4
    lamdba_warmup_steps: 10000
    requirements: duration=6 days & cuda(mem=24G)

retrieval:
    requirements: duration=6 days & cuda(mem=24G)
    topK: 1000
    retTopK: 50
    batch_size_full_retriever: 200
