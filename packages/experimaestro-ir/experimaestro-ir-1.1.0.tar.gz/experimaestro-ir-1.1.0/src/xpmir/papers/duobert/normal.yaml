id: duobert
title: "DuoBERT trained on MS-Marco"
description: |
    DuoBERT model

        R. Nogueira, W. Yang, K. Cho, et J. Lin, « Multi-Stage Document Ranking with BERT », arXiv:1910.14424 [cs], oct. 2019. http://arxiv.org/abs/1910.14424


gpu: true
indexation:
    requirements: duration=6 days & cpu(mem=4G, cores=8)

validation:
    size: 500

retrieval:
    requirements: duration=2 days & cuda(mem=24G)
    k: 100
    base_k: 50

monobert:
    requirements: duration=4 days & cuda(mem=24G) * 2

    optimization:
        steps_per_epoch: 32
        batch_size: 64
        max_epochs: 3200
        num_warmup_steps: 10000
        warmup_min_factor: 0
        lr: 3.0e-6
        weight_decay: .01

    validation_interval: 32

duobert:
    requirements: duration=4 days & cuda(mem=24G) * 2

    optimization:
        # Train on 100k iterations
        max_epochs: 1_000
        steps_per_epoch: 100

        # Learning rate warmup over the first 10,000 steps, and linear decay of the learning rate
        num_warmup_steps: 10_000
        batch_size: 64
        warmup_min_factor: 0
        lr: 3.0e-6

    # Validate 20 times over the 3200 epochs
    validation_interval: 50
    base_validation_top_k: 1000
    validation_top_k: 50
