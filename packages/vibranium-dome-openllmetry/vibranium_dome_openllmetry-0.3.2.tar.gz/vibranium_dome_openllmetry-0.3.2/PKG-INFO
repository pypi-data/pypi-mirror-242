Metadata-Version: 2.1
Name: vibranium-dome-openllmetry
Version: 0.3.2
Summary: Vibranium Dome LLM OpenTelemetry for Python
Home-page: https://github.com/genia-dev/openllmetry
License: Apache-2.0
Author: Uri Shamay
Author-email: cmpxchg16@gmail.com
Requires-Python: >=3.8.1,<4
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: colorama (>=0.4.6,<0.5.0)
Requires-Dist: deprecated (>=1.2.14,<2.0.0)
Requires-Dist: jinja2 (>=3.1.2,<4.0.0)
Requires-Dist: opentelemetry-api (>=1.21.0,<2.0.0)
Requires-Dist: opentelemetry-exporter-otlp-proto-grpc (>=1.20.0,<2.0.0)
Requires-Dist: opentelemetry-exporter-otlp-proto-http (>=1.20.0,<2.0.0)
Requires-Dist: opentelemetry-instrumentation-anthropic (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-chromadb (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-cohere (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-langchain (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-llamaindex (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-openai (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-pinecone (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-pymysql (>=0.42b0,<0.43)
Requires-Dist: opentelemetry-instrumentation-requests (>=0.42b0,<0.43)
Requires-Dist: opentelemetry-instrumentation-transformers (>=0.3.2,<0.4.0)
Requires-Dist: opentelemetry-instrumentation-urllib3 (>=0.42b0,<0.43)
Requires-Dist: opentelemetry-sdk (>=1.20.0,<2.0.0)
Requires-Dist: opentelemetry-semantic-conventions-ai (>=0.0.7,<0.0.8)
Requires-Dist: posthog (>=3.0.2,<4.0.0)
Requires-Dist: pydantic (>=2.5.0,<3.0.0)
Requires-Dist: tenacity (>=8.2.3,<9.0.0)
Project-URL: Repository, https://github.com/genia-dev/openllmetry
Description-Content-Type: text/markdown

# traceloop-sdk

Traceloopâ€™s Python SDK allows you to easily start monitoring and debugging your LLM execution. Tracing is done in a non-intrusive way, built on top of OpenTelemetry. You can choose to export the traces to Traceloop, or to your existing observability stack.

```python
Traceloop.init(app_name="joke_generation_service")

@workflow(name="joke_creation")
def create_joke():
    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Tell me a joke about opentelemetry"}],
    )

    return completion.choices[0].message.content
```

