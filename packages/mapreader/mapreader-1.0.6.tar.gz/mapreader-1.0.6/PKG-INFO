Metadata-Version: 2.1
Name: mapreader
Version: 1.0.6
Summary: A computer vision pipeline for the semantic exploration of maps/images at scale
Home-page: https://github.com/Living-with-machines/MapReader
Download-URL: https://github.com/Living-with-machines/MapReader/archive/refs/heads/main.zip
Author: MapReader team
License: MIT License
Keywords: Computer Vision,Classification,Deep Learning,living with machines
Platform: OS Independent
Classifier: Development Status :: 3 - Alpha
Classifier: License :: OSI Approved :: MIT License
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Intended Audience :: End Users/Desktop
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: Operating System :: Unix
Classifier: Operating System :: Microsoft :: Windows
Classifier: Operating System :: MacOS
Classifier: Operating System :: OS Independent
Classifier: Topic :: Software Development
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.7, <3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: matplotlib<4.0.0,>=3.5.0
Requires-Dist: numpy<2.0.0,>=1.21.5
Requires-Dist: pandas<2.0.0,>=1.3.4
Requires-Dist: pyproj<4.0.0,>=3.2.0
Requires-Dist: azure-storage-blob<13.0.0,>=12.9.0
Requires-Dist: aiohttp<4.0.0,>=3.8.1
Requires-Dist: Shapely<3.0.0,>=2.0.0
Requires-Dist: nest-asyncio<2.0.0,>=1.5.1
Requires-Dist: scikit-image>=0.18.3
Requires-Dist: scikit-learn<2.0.0,>=1.0.1
Requires-Dist: torch<2.0.0,>=1.10.0
Requires-Dist: torchvision<0.12.1,>=0.11.1
Requires-Dist: jupyter<2.0.0,>=1.0.0
Requires-Dist: ipykernel<7.0.0,>=6.5.1
Requires-Dist: ipywidgets<8.0.0,>=7.7.3
Requires-Dist: ipyannotate==0.1.0-beta.0
Requires-Dist: Cython<0.30.0,>=0.29.24
Requires-Dist: PyYAML<7.0,>=6.0
Requires-Dist: tensorboard<3.0.0,>=2.7.0
Requires-Dist: parhugin<0.0.4,>=0.0.3
Requires-Dist: geopy==2.1.0
Requires-Dist: rasterio<2.0.0,>=1.2.10
Requires-Dist: keplergl<0.4.0,>=0.3.2
Requires-Dist: simplekml<2.0.0,>=1.3.6
Requires-Dist: versioneer>=0.28
Requires-Dist: tqdm<5.0.0
Requires-Dist: torchinfo<2.0.0
Requires-Dist: openpyxl<4.0.0
Provides-Extra: dev
Requires-Dist: pytest<8.0.0; extra == "dev"
Requires-Dist: pytest-cov<5.0.0,>=4.1.0; extra == "dev"
Requires-Dist: timm<1.0.0; extra == "dev"
Requires-Dist: transformers<5.0.0; extra == "dev"
Requires-Dist: black<24.0.0,>=23.7.0; extra == "dev"
Requires-Dist: flake8<7.0.0,>=6.0.0; extra == "dev"

<div align="center">
    <br>
    <p align="center">
    <h1>MapReader</h1>
    <h2>A computer vision pipeline for exploring and analyzing images at scale</h2>
    </p>
</div>

<p align="center">
    <a href="https://pypi.org/project/mapreader/">
        <img alt="PyPI" src="https://img.shields.io/pypi/v/MapReader">
    </a>
    <a href="https://github.com/Living-with-machines/MapReader/blob/main/LICENSE">
        <img alt="License" src="https://img.shields.io/badge/License-MIT-yellow.svg">
    </a>
    <a href="https://github.com/Living-with-machines/MapReader/actions/workflows/mr_ci.yml/badge.svg">
        <img alt="Integration Tests badge" src="https://github.com/Living-with-machines/MapReader/actions/workflows/mr_ci.yml/badge.svg">
    </a>
    <a href="https://zenodo.org/badge/latestdoi/430661738"><img src="https://zenodo.org/badge/430661738.svg" alt="DOI"></a>
    <br/>
</p>

## What is MapReader?

<div align="center">
    <figure>
    <img src="https://raw.githubusercontent.com/Living-with-machines/MapReader/main/figs/river_banner_8bit.png"
        alt="Annotated Map with Prediction Outputs"
        width="70%">
    </figure>
</div>

MapReader is an end-to-end computer vision (CV) pipeline for exploring and analyzing images at scale.

MapReader was developed in the [Living with Machines](https://livingwithmachines.ac.uk/) project to analyze large collections of historical maps but is a _**generalizable**_ computer vision pipeline which can be applied to _**any images**_ in a wide variety of domains.

## Overview

MapReader is a groundbreaking interdisciplinary tool that emerged from a specific set of geospatial historical research questions. It was inspired by methods in biomedical imaging and geographic information science, which were adapted for use by historians, for example in our [Journal of Victorian Culture](https://doi.org/10.1093/jvcult/vcab009) and [Geospatial Humanities 2022 SIGSPATIAL workshop](https://arxiv.org/abs/2111.15592) papers. The success of the tool subsequently generated interest from plant phenotype researchers working with large image datasets, and so MapReader is an example of cross-pollination between the humanities and the sciences made possible by reproducible data science.

### MapReader pipeline

<div align="center">
  <figure>
  <img src="https://raw.githubusercontent.com/Living-with-machines/MapReader/main/docs/source/figures/pipeline_explained.png"
        alt="MapReader pipeline"
        width="70%">
  </figure>
</div>

The MapReader pipeline consists of a linear sequence of tasks which, together, can be used to train a computer vision (CV) classifier to recognize visual features within maps and identify patches containing these features across entire map collections.

See our [About MapReader](https://mapreader.readthedocs.io/en/latest/About.html) page to learn more.

## Documentation

The MapReader documentation can be found at https://mapreader.readthedocs.io/en/latest/index.html.

**New users** should refer to the [Installation instructions](https://mapreader.readthedocs.io/en/latest/Install.html) and [Input guidance](https://mapreader.readthedocs.io/en/latest/Input-guidance.html) for help with the initial set up of MapReader.

**All users** should refer to our [User Guide](https://mapreader.readthedocs.io/en/latest/User-guide/User-guide.html) for guidance on how to use MapReader. This contains end-to-end instructions on how to use the MapReader pipeline, plus a number of worked examples illustrating use cases such as:

- Geospatial images (i.e. maps)
- Non-geospatial images

 **Developers and contributors** may also want to refer to the [API documentation](https://mapreader.readthedocs.io/en/latest/api/index.html) and [Contribution guide](https://mapreader.readthedocs.io/en/latest/Contribution-guide.html) for guidance on how to contribute to the MapReader package.

**Join our Slack workspace!**
Please fill out [this form](https://forms.gle/dXjECHZQkwrZ3Xpt9) to receive an invitation to the Slack workspace.

## What is included in this repo?

The MapReader package provides a set of tools to:

- **Download** images/maps and metadata stored on web-servers (e.g. tileservers which can be used to retrieve maps from OpenStreetMap (OSM), the National Library of Scotland (NLS), or elsewhere).
- **Load** images/maps and metadata stored locally.
- **Pre-process** images/maps:
  - patchify (create patches from a parent image),
  - resample (use image transformations to alter pixel-dimensions/resolution/orientation/etc.),
  - remove borders outside the neatline,
  - reproject between coordinate reference systems (CRS).
- **Annotate** images/maps (or their patches) using an interactive annotation tool.
- **Train or fine-tune** Computer Vision (CV) models and use these to **predict** labels (i.e. model inference) on large sets of images/maps.

Various **plotting and analysis** functionalities are also included (based on packages such as _matplotlib_, _cartopy_, _Google Earth_, and _[kepler.gl](https://kepler.gl/))_.

## How to cite MapReader

If you use MapReader in your work, please cite both the MapReader repo and [our SIGSPATIAL paper](https://dl.acm.org/doi/10.1145/3557919.3565812):

- Kasra Hosseini, Daniel C. S. Wilson, Kaspar Beelen, and Katherine McDonough. 2022. MapReader: a computer vision pipeline for the semantic exploration of maps at scale. In Proceedings of the 6th ACM SIGSPATIAL International Workshop on Geospatial Humanities (GeoHumanities '22). Association for Computing Machinery, New York, NY, USA, 8–19. https://doi.org/10.1145/3557919.3565812
- Kasra Hosseini, Rosie Wood, Andy Smith, Katie McDonough, Daniel C.S. Wilson, Christina Last, Kalle Westerling, and Evangeline Mae Corcoran. “Living-with-machines/mapreader: End of Lwm”. Zenodo, July 27, 2023. https://doi.org/10.5281/zenodo.8189653.


## Acknowledgements

This work was supported by Living with Machines (AHRC grant AH/S01179X/1) and The Alan Turing Institute (EPSRC grant EP/N510129/1).

Living with Machines, funded by the UK Research and Innovation (UKRI) Strategic Priority Fund, is a multidisciplinary collaboration delivered by the Arts and Humanities Research Council (AHRC), with The Alan Turing Institute, the British Library and the Universities of Cambridge, East Anglia, Exeter, and Queen Mary University of London.

Maps above reproduced with the permission of the National Library of Scotland https://maps.nls.uk/index.html
